{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T06:47:40.671418Z","iopub.status.busy":"2024-03-07T06:47:40.670569Z","iopub.status.idle":"2024-03-07T06:47:59.444697Z","shell.execute_reply":"2024-03-07T06:47:59.443536Z","shell.execute_reply.started":"2024-03-07T06:47:40.671371Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import cv2\n","from glob import glob\n","import tensorflow as tf\n","import tensorflow.keras as keras\n","import keras.backend as K\n","import tensorflow.keras.layers as L\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, MaxPool2D, Add, Dropout, Concatenate, Conv2DTranspose, Dense, Reshape, Flatten, Softmax, Lambda, UpSampling2D, AveragePooling2D, Activation, BatchNormalization, GlobalAveragePooling2D, SeparableConv2D\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.metrics import MeanIoU\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.metrics import BinaryAccuracy, Precision, Recall\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.applications import DenseNet121\n","import pandas as pd\n","import tensorflow_probability as tfp"]},{"cell_type":"markdown","metadata":{},"source":["## Data Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T06:47:59.447521Z","iopub.status.busy":"2024-03-07T06:47:59.446851Z","iopub.status.idle":"2024-03-07T06:47:59.458515Z","shell.execute_reply":"2024-03-07T06:47:59.457425Z","shell.execute_reply.started":"2024-03-07T06:47:59.447487Z"},"trusted":true},"outputs":[],"source":["def load_image(path, size, mask=False):\n","    image = Image.open(path)\n","    image = image.resize((size, size))\n","\n","    if mask:\n","        image = image.convert('L')  # Convert to grayscale\n","    else:\n","        image = image.convert('RGB')  # Convert to RGB\n","    \n","    image = np.array(image)\n","    return image\n","\n","def load_data(root_path, size):\n","    images = []\n","    masks = []\n","\n","    image_folder = os.path.join(root_path, 'masks')\n","    mask_folder = os.path.join(root_path, 'images')\n","\n","    for image_path in sorted(glob(os.path.join(image_folder, '*png'))):\n","        img_id = os.path.basename(image_path).split('.')[0]\n","        mask_path = os.path.join(mask_folder, f'{img_id}.png')\n","\n","        img = load_image(image_path, size) / 255.0\n","        mask = load_image(mask_path, size, mask=True) / 255.0\n","\n","        images.append(img)\n","        masks.append(mask)\n","\n","    return np.array(images), np.array(masks)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T06:47:59.460054Z","iopub.status.busy":"2024-03-07T06:47:59.459694Z","iopub.status.idle":"2024-03-07T06:48:02.331389Z","shell.execute_reply":"2024-03-07T06:48:02.330180Z","shell.execute_reply.started":"2024-03-07T06:47:59.460023Z"},"trusted":true},"outputs":[],"source":["size = 512   # image size: 512x512\n","root_path = '/kaggle/input/tnbc-seg/TNBC_NucleiSegmentation/TNBC_NucleiSegmentation'\n","X_train, y_train = load_data(root_path, size)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T06:48:02.335829Z","iopub.status.busy":"2024-03-07T06:48:02.335040Z","iopub.status.idle":"2024-03-07T06:48:02.341972Z","shell.execute_reply":"2024-03-07T06:48:02.340888Z","shell.execute_reply.started":"2024-03-07T06:48:02.335792Z"},"trusted":true},"outputs":[],"source":["print(f\"X shape: {X_train.shape}     |  y shape: {y_train.shape}\")\n","\n","# prepare data to modeling\n","# X = np.expand_dims(X, -1)\n","y_train = np.expand_dims(y_train, -1)\n","\n","print(f\"\\nX shape: {X_train.shape}  |  y shape: {y_train.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T06:48:02.344530Z","iopub.status.busy":"2024-03-07T06:48:02.343968Z","iopub.status.idle":"2024-03-07T06:48:02.605186Z","shell.execute_reply":"2024-03-07T06:48:02.604042Z","shell.execute_reply.started":"2024-03-07T06:48:02.344488Z"},"trusted":true},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.1, random_state=35)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=35)\n","\n","print('X_train shape:',X_train.shape)\n","print('y_train shape:',y_train.shape)\n","print('X_val shape:',X_val.shape)\n","print('X_test shape:',X_test.shape)\n","print('y_test shape:',y_test.shape)"]},{"cell_type":"markdown","metadata":{},"source":["#### Mask Image pair"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T06:48:02.606748Z","iopub.status.busy":"2024-03-07T06:48:02.606423Z","iopub.status.idle":"2024-03-07T06:48:02.947543Z","shell.execute_reply":"2024-03-07T06:48:02.946415Z","shell.execute_reply.started":"2024-03-07T06:48:02.606715Z"},"trusted":true},"outputs":[],"source":["image = X_test[0]\n","mask = y_test[0]\n","\n","fig, axes = plt.subplots(1, 2, figsize=(5, 2))\n","axes[0].imshow(image, cmap='gray')\n","axes[0].axis('off')\n","axes[0].set_title('Image')\n","\n","axes[1].imshow(mask*255, cmap='gray', vmin=0, vmax=1)\n","axes[1].axis('off')\n","axes[1].set_title('Mask')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Metrics and Losses"]},{"cell_type":"markdown","metadata":{},"source":["#### Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T06:48:02.949261Z","iopub.status.busy":"2024-03-07T06:48:02.948922Z","iopub.status.idle":"2024-03-07T06:48:02.964382Z","shell.execute_reply":"2024-03-07T06:48:02.963215Z","shell.execute_reply.started":"2024-03-07T06:48:02.949227Z"},"trusted":true},"outputs":[],"source":["def dice_score(y_true, y_pred):\n","    smooth = K.epsilon()\n","    y_true_flat = K.flatten(K.cast(y_true, 'float32'))\n","    y_pred_flat = K.flatten(y_pred)\n","    intersection = K.sum(y_true_flat * y_pred_flat)\n","    score = (2. * intersection + smooth) / (K.sum(y_true_flat) + K.sum(y_pred_flat) + smooth)\n","    return score\n","\n","def iou(y_true, y_pred):\n","    smooth = K.epsilon()\n","    y_true_flat = K.flatten(K.cast(y_true, 'float32'))\n","    y_pred_flat = K.flatten(y_pred)\n","    intersection = K.sum(y_true_flat * y_pred_flat)\n","    union = K.sum(y_true_flat) + K.sum(y_pred_flat) - intersection + smooth\n","    iou = (intersection + smooth) / union\n","    return iou\n","\n","def recall(y_true, y_pred):\n","    smooth = K.epsilon()\n","    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n","    y_true_flat = K.flatten(K.cast(y_true, 'float32'))\n","    y_pred_flat = K.flatten(y_pred_pos)\n","    tp = K.sum(y_true_flat * y_pred_flat)\n","    fn = K.sum(y_true_flat * (1 - y_pred_flat))\n","    recall = (tp + smooth) / (tp + fn + smooth)\n","    return recall\n","\n","def precision(y_true, y_pred):\n","    smooth = K.epsilon()\n","    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n","    y_true_flat = K.flatten(K.cast(y_true, 'float32'))\n","    y_pred_flat = K.flatten(y_pred_pos)\n","    tp = K.sum(y_true_flat * y_pred_flat)\n","    fp = K.sum((1 - y_true_flat) * y_pred_flat)\n","    precision = (tp + smooth) / (tp + fp + smooth)\n","    return precision"]},{"cell_type":"markdown","metadata":{},"source":["#### Losses"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T06:48:02.965953Z","iopub.status.busy":"2024-03-07T06:48:02.965625Z","iopub.status.idle":"2024-03-07T06:48:02.981033Z","shell.execute_reply":"2024-03-07T06:48:02.980185Z","shell.execute_reply.started":"2024-03-07T06:48:02.965911Z"},"trusted":true},"outputs":[],"source":["def dice_loss(y_true, y_pred):\n","    loss = 1 - dice_score(y_true, y_pred)\n","    return loss\n","\n","def iou_loss(y_true, y_pred):\n","    loss = 1 - iou(y_true, y_pred)\n","    return loss\n","    \n","def focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n","    epsilon = tf.keras.backend.epsilon()\n","    y_pred = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n","    y_true = tf.cast(y_true, tf.float32)\n","    pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n","    focal_weight = alpha * tf.pow(1 - pt, gamma)\n","    loss = tf.reduce_mean(-focal_weight * tf.math.log(pt))\n","    return loss\n","\n","def bce_loss(y_true, y_pred):\n","    y_true = tf.cast(y_true, tf.float32)\n","    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, logits=y_pred))\n","    return loss\n","\n","def combined_loss(y_true, y_pred):\n","    loss = dice_loss(y_true, y_pred) + bce_loss(y_true, y_pred)\n","    return loss"]},{"cell_type":"markdown","metadata":{},"source":["## Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T06:48:02.983240Z","iopub.status.busy":"2024-03-07T06:48:02.982837Z","iopub.status.idle":"2024-03-07T06:48:15.698241Z","shell.execute_reply":"2024-03-07T06:48:15.697173Z","shell.execute_reply.started":"2024-03-07T06:48:02.983192Z"},"trusted":true},"outputs":[],"source":["'''texts = [\n","    \"tumor epithelial tissue\",\n","    \"necrotic tissue\",\n","    \"lymphocytic tissue\",\n","    \"tumor associated stromal tissue\",\n","    \"coagulative necrosis\",\n","    \"liquefactive necrosis\",\n","    \"desmoplasia\",\n","    \"granular and non granular leukocytes\",\n","    \"perinuclear halo\",\n","    \"interstitial space\",\n","    \"neutrophils\",\n","    \"macrophages\",\n","    \"collagen\",\n","    \"fibronectin\",\n","    \"hyperplasia\",\n","    \"dysplasia\"\n","]'''\n","\n","text = pd.read_csv('/kaggle/input/tnbc-seg/text_labels.csv', header=None)\n","text = tf.convert_to_tensor(np.asarray(text), dtype=tf.float32)\n","\n","text = Dense(32, activation='relu')(text)\n","text = Dense(32, activation='relu')(tf.transpose(text, perm=[1,0]))\n","text = tf.expand_dims(text, axis=0)\n","text = tf.expand_dims(text, axis=-1)\n","\n","class DistributionModel(tf.keras.Model):\n","    def __init__(self, text):\n","        super(DistributionModel, self).__init__()\n","        self.mean_layer = tf.keras.layers.SeparableConv2D(filters=1, kernel_size=1, padding='same', activation='softplus')\n","        self.stddev_layer = tf.keras.layers.SeparableConv2D(filters=1, kernel_size=1, padding='same', activation='softplus')\n","        self.distribution_layer = tfp.layers.DistributionLambda(\n","            lambda t: tfp.distributions.Normal(loc=t[..., :1], scale=t[..., 1:])\n","        )\n","        self.concat = tf.keras.layers.Concatenate(axis=-1)\n","        self.conv_t1 = tf.keras.layers.Conv2DTranspose(256, 2, strides=2, padding=\"same\", activation='relu')\n","        self.conv_t2 = tf.keras.layers.Conv2DTranspose(256, 2, strides=2, padding=\"same\", activation='relu')\n","        self.conv_t3 = tf.keras.layers.Conv2DTranspose(256, 2, strides=2, padding=\"same\", activation='relu')\n","        self.conv_t4 = tf.keras.layers.Conv2DTranspose(256, 2, strides=2, padding=\"same\", activation='relu')\n","        self.text = text\n","        self.conv_tt1 = tf.keras.layers.Conv2DTranspose(1, 2, strides=2, padding=\"same\", activation='relu')\n","        self.conv_tt2 = tf.keras.layers.Conv2DTranspose(1, 2, strides=2, padding=\"same\", activation='relu')\n","        self.conv_tt3 = tf.keras.layers.Conv2DTranspose(1, 2, strides=2, padding=\"same\", activation='relu')\n","        self.conv_tt4 = tf.keras.layers.Conv2DTranspose(1, 2, strides=2, padding=\"same\", activation='relu')\n","\n","    def distribution(self, x, text):\n","        mean = self.mean_layer(x)+tf.math.reduce_mean(text, axis=[-1,-2,-3])\n","        stddev = tf.math.sqrt(tf.math.square(self.stddev_layer(x))+tf.math.reduce_variance(text, axis=[-1,-2,-3]))\n","\n","        # Concatenate mean and standard deviation\n","        parameters = self.concat([mean, stddev])\n","\n","        # Generate distribution\n","        distribution = self.distribution_layer(parameters)\n","        return distribution\n","\n","    def distribution_attn(self, x):\n","        x1 = self.conv_t1(x)\n","        x2 = self.conv_t2(x1)\n","        x3 = self.conv_t3(x2)\n","        x4 = self.conv_t4(x3)\n","        text = self.conv_tt1(self.text)\n","        dis1 = self.distribution(x1, text)\n","        text = self.conv_tt2(text)\n","        dis2 = self.distribution(x2, text)\n","        text = self.conv_tt3(text)\n","        dis3 = self.distribution(x3, text)\n","        text = self.conv_tt4(text)\n","        dis4 = self.distribution(x4, text)\n","        return dis1, dis2, dis3, dis4\n","\n","    def call(self, inputs):\n","        return self.distribution_attn(inputs)\n","\n","def conv_block(x, num_filters, kernel_size, padding=\"same\", act=True):\n","    x = Conv2D(num_filters, kernel_size, padding=padding, use_bias=False)(x)\n","    x = BatchNormalization()(x)\n","    if act:\n","        x = Activation(\"relu\")(x)\n","    return x\n","\n","def multires_block(x, num_filters, alpha=1.67):\n","    W = num_filters * alpha\n","\n","    x0 = x\n","    x1 = conv_block(x0, int(W*0.167), 3)\n","    x2 = conv_block(x1, int(W*0.333), 3)\n","    x3 = conv_block(x2, int(W*0.5), 3)\n","    xc = Concatenate()([x1, x2, x3])\n","    xc = BatchNormalization()(xc)\n","\n","    nf = int(W*0.167) + int(W*0.333) + int(W*0.5)\n","    sc = conv_block(x0, nf, 1, act=False)\n","\n","    x = Activation(\"relu\")(xc + sc)\n","    x = BatchNormalization()(x)\n","    return x\n","\n","def res_path(x, num_filters, length): \n","    check = L.GlobalMaxPooling2D()(x)\n","    check = L.Dense(1, activation='sigmoid')(x)\n","    check = tf.math.reduce_mean(check, axis=0)\n","    \n","    x01 = x\n","    x11 = conv_block(x01, num_filters, 3, act=False)\n","    sc1 = conv_block(x01, num_filters, 1, act=False)\n","    x = Activation(\"relu\")(x11 + sc1)\n","    x = BatchNormalization()(x)\n","    \n","    x02 = Concatenate()([x,x01])\n","    x12 = conv_block(x02, num_filters, 3, act=False)\n","    sc2 = conv_block(x02, num_filters, 1, act=False)\n","    x = Activation(\"relu\")(x12 + sc2)\n","    x = BatchNormalization()(x)\n","    \n","    x03 = Concatenate()([x,x01,x02])\n","    x13 = conv_block(x03, num_filters, 3, act=False)\n","    sc3 = conv_block(x03, num_filters, 1, act=False)\n","    x = Activation(\"relu\")(x13 + sc3)\n","    x = BatchNormalization()(x)\n","    \n","    x04 = Concatenate()([x,x01,x02,x03])\n","    x14 = conv_block(x04, num_filters, 3, act=False)\n","    sc4 = conv_block(x04, num_filters, 1, act=False)\n","    x = Activation(\"relu\")(x14 + sc4)\n","    x = BatchNormalization()(x)\n","    return x*check\n","\n","def encoder_block(x, num_filters, length):\n","    x = multires_block(x, num_filters)\n","    s = res_path(x, num_filters, length)\n","    p = MaxPooling2D((2, 2))(x)\n","    return s, p\n","\n","def decoder_block(x, skip, num_filters):\n","    x = Conv2DTranspose(num_filters, 2, strides=2, padding=\"same\")(x)\n","    x = Concatenate()([x, skip])\n","    x = multires_block(x, num_filters)\n","    return x\n","\n","def build_multiresunet(shape, text):\n","    \"\"\" Input \"\"\"\n","    inputs = Input(shape)\n","\n","    \"\"\" Encoder \"\"\"\n","    p0 = inputs\n","    s1, p1 = encoder_block(p0, 32, 4)\n","    s2, p2 = encoder_block(p1, 64, 4)\n","    s3, p3 = encoder_block(p2, 128, 4)\n","    s4, p4 = encoder_block(p3, 256, 4)\n","\n","    \"\"\" Bridge \"\"\"\n","    b1 = multires_block(p4, 512)\n","    dis1, dis2, dis3, dis4 = DistributionModel(text)(b1)\n","\n","    \"\"\" Decoder \"\"\"\n","    d1 = decoder_block(b1, s4, 256)\n","    d1 = d1*dis1\n","    d2 = decoder_block(d1, s3, 128)\n","    d2 = d2*dis2\n","    d3 = decoder_block(d2, s2, 64)\n","    d3 = d3*dis3\n","    d4 = decoder_block(d3, s1, 32)\n","    d4 = d4*dis4\n","    \n","    \"\"\" Output \"\"\"\n","    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n","\n","    \"\"\" Model \"\"\"\n","    model = Model(inputs, outputs, name=\"MultiResUNET\")\n","\n","    return model\n","    \n","model = build_multiresunet((512, 512, 3), text)\n","optimizer = Adam(learning_rate=0.0001)\n","model.compile(loss=combined_loss, metrics=[\"accuracy\", dice_score, recall, precision, iou], optimizer=optimizer)\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T06:48:15.701740Z","iopub.status.busy":"2024-03-07T06:48:15.701444Z","iopub.status.idle":"2024-03-07T07:33:49.091322Z","shell.execute_reply":"2024-03-07T07:33:49.090482Z","shell.execute_reply.started":"2024-03-07T06:48:15.701712Z"},"trusted":true},"outputs":[],"source":["model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n","    filepath='/kaggle/working/model.h5',\n","    monitor='val_dice_score',\n","    save_best_only=True,\n","    save_weights_only=True,\n","    mode='max',\n","    verbose=1\n","    )\n","\n","history = model.fit(X_train, y_train,\n","                    epochs = 100,\n","                    batch_size = 2,\n","                    validation_data = (X_val,y_val),\n","                    verbose = 1,\n","                    callbacks=[model_checkpoint_callback],\n","                    shuffle = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T07:33:49.093108Z","iopub.status.busy":"2024-03-07T07:33:49.092786Z","iopub.status.idle":"2024-03-07T07:33:50.604390Z","shell.execute_reply":"2024-03-07T07:33:50.603307Z","shell.execute_reply.started":"2024-03-07T07:33:49.093077Z"},"trusted":true},"outputs":[],"source":["def Train_Val_Plot(loss, val_loss, dice_score, val_dice_score, iou, val_iou, recall, val_recall, precision, val_precision, accuracy, val_accuracy):\n","    fig, axs = plt.subplots(2, 3, figsize=(20, 10))\n","    fig.suptitle(\"MODEL'S METRICS VISUALIZATION\")\n","\n","    # Loss plot\n","    axs[0, 0].plot(range(1, len(loss) + 1), loss)\n","    axs[0, 0].plot(range(1, len(val_loss) + 1), val_loss)\n","    axs[0, 0].set_title('History of Loss')\n","    axs[0, 0].set_xlabel('Epochs')\n","    axs[0, 0].set_ylabel('Loss')\n","    axs[0, 0].legend(['training', 'validation'])\n","\n","    # Dice Coefficient plot\n","    axs[0, 1].plot(range(1, len(dice_score) + 1), dice_score)\n","    axs[0, 1].plot(range(1, len(val_dice_score) + 1), val_dice_score)\n","    axs[0, 1].set_title('History of Dice Coefficient')\n","    axs[0, 1].set_xlabel('Epochs')\n","    axs[0, 1].set_ylabel('Dice Coefficient')\n","    axs[0, 1].legend(['training', 'validation'])\n","\n","    # Mean IOU plot\n","    axs[0, 2].plot(range(1, len(iou) + 1), iou)\n","    axs[0, 2].plot(range(1, len(val_iou) + 1), val_iou)\n","    axs[0, 2].set_title('History of IOU')\n","    axs[0, 2].set_xlabel('Epochs')\n","    axs[0, 2].set_ylabel('IOU')\n","    axs[0, 2].legend(['training', 'validation'])\n","\n","    # Recall plot\n","    axs[1, 0].plot(range(1, len(recall) + 1), recall)\n","    axs[1, 0].plot(range(1, len(val_recall) + 1), val_recall)\n","    axs[1, 0].set_title('History of Recall')\n","    axs[1, 0].set_xlabel('Epochs')\n","    axs[1, 0].set_ylabel('Recall')\n","    axs[1, 0].legend(['training', 'validation'])\n","\n","    # Precision plot\n","    axs[1, 1].plot(range(1, len(precision) + 1), precision)\n","    axs[1, 1].plot(range(1, len(val_precision) + 1), val_precision)\n","    axs[1, 1].set_title('History of Precision')\n","    axs[1, 1].set_xlabel('Epochs')\n","    axs[1, 1].set_ylabel('Precision')\n","    axs[1, 1].legend(['training', 'validation'])\n","\n","    # Accuracy plot\n","    axs[1, 2].plot(range(1, len(accuracy) + 1), accuracy)\n","    axs[1, 2].plot(range(1, len(val_accuracy) + 1), val_accuracy)\n","    axs[1, 2].set_title('History of Accuracy')\n","    axs[1, 2].set_xlabel('Epochs')\n","    axs[1, 2].set_ylabel('Accuracy')\n","    axs[1, 2].legend(['training', 'validation'])\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","Train_Val_Plot(\n","    history.history['loss'], history.history['val_loss'],\n","    history.history['dice_score'], history.history['val_dice_score'],\n","    history.history['iou'], history.history['val_iou'],\n","    history.history['recall'], history.history['val_recall'],\n","    history.history['precision'], history.history['val_precision'],\n","    history.history['accuracy'], history.history['val_accuracy']\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T07:33:50.606864Z","iopub.status.busy":"2024-03-07T07:33:50.606066Z","iopub.status.idle":"2024-03-07T07:34:10.987672Z","shell.execute_reply":"2024-03-07T07:34:10.986614Z","shell.execute_reply.started":"2024-03-07T07:33:50.606823Z"},"trusted":true},"outputs":[],"source":["model.load_weights(\"/kaggle/working/model.h5\")\n","loss, accuracy, dice, recall, precision, iou = model.evaluate(X_test, y_test, batch_size = 4, verbose = 0)\n","print('loss:', loss)\n","print('accuracy:', accuracy)\n","print('dice:', dice)\n","print('recall:', recall)\n","print('precision:', precision)\n","print('iou:', iou)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T07:34:10.990007Z","iopub.status.busy":"2024-03-07T07:34:10.989175Z","iopub.status.idle":"2024-03-07T07:34:11.019392Z","shell.execute_reply":"2024-03-07T07:34:11.018183Z","shell.execute_reply.started":"2024-03-07T07:34:10.989961Z"},"trusted":true},"outputs":[],"source":["modeller = Model(inputs=model.input, outputs=[model.get_layer(name=\"tf.__operators__.add_17\").output,model.get_layer(name=\"tf.__operators__.add_20\").output,model.layers[-3].output])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T07:34:11.021931Z","iopub.status.busy":"2024-03-07T07:34:11.021084Z","iopub.status.idle":"2024-03-07T07:34:21.529449Z","shell.execute_reply":"2024-03-07T07:34:21.527752Z","shell.execute_reply.started":"2024-03-07T07:34:11.021885Z"},"trusted":true},"outputs":[],"source":["for z in range(0,15):\n","    # Load one image and corresponding mask from the test dataset\n","    test_image = X_test[z]  # Replace X_test with your actual test dataset\n","    test_mask = y_test[z]  # Replace y_test with your actual test masks\n","\n","    # Reshape the image to match the input shape of the model\n","    test_image = np.reshape(test_image, (1,) + test_image.shape)\n","\n","    # Predict the segmentation mask for the test image\n","    predicted_mask = model.predict(test_image)[0]\n","    feature_map1, feature_map2, feature_map3 = modeller.predict(test_image)\n","\n","    # Convert the predicted mask values to binary (0 or 1)\n","    predicted_mask_binary = np.where(predicted_mask > 0.5, 1, 0) * 255\n","\n","    # Create subplots\n","    fig, axes = plt.subplots(1, len(feature_map1) + 5, figsize=(20, 4))\n","\n","    # Plot the test image\n","    axes[0].imshow(test_image[0], cmap='gray')\n","    axes[0].set_title('Test Image')\n","    axes[0].axis('off')\n","\n","    # Plot the ground truth mask\n","    axes[1].imshow(test_mask, cmap='gray')\n","    axes[1].set_title('Ground Truth Mask')\n","    axes[1].axis('off')\n","\n","    # Plot the predicted mask\n","    axes[2].imshow(predicted_mask_binary, cmap='gray')\n","    axes[2].set_title('Predicted Mask')\n","    axes[2].axis('off')\n","\n","    # Plot the feature maps (Set 1)\n","    for i, fmap in enumerate(feature_map1):\n","        axes[i + 3].imshow(fmap[:, :, 0], cmap='jet')\n","        axes[i + 3].set_title(f'Encoder last layer')\n","        axes[i + 3].axis('off')\n","\n","    # Plot the feature maps (Set 2)\n","    for i, fmap in enumerate(feature_map2):\n","        axes[i + 3 + len(feature_map1)].imshow(fmap[:, :, 0], cmap='jet')\n","        axes[i + 3 + len(feature_map1)].set_title(f'Bottleneck')\n","        axes[i + 3 + len(feature_map1)].axis('off')\n","\n","    # Plot the feature maps (Set 3)\n","    for i, fmap in enumerate(feature_map3):\n","        axes[i + 3 + len(feature_map1) + len(feature_map2)].imshow(fmap[:, :, 0], cmap='jet')\n","        axes[i + 3 + len(feature_map1) + len(feature_map2)].set_title(f'Decoder last layer')\n","        axes[i + 3 + len(feature_map1) + len(feature_map2)].axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T07:34:29.556932Z","iopub.status.busy":"2024-03-07T07:34:29.556551Z","iopub.status.idle":"2024-03-07T07:34:29.582041Z","shell.execute_reply":"2024-03-07T07:34:29.581226Z","shell.execute_reply.started":"2024-03-07T07:34:29.556895Z"},"trusted":true},"outputs":[],"source":["modeller = Model(inputs=model.input, outputs=[model.get_layer(name=\"tf.math.multiply\").output,model.get_layer(name=\"tf.math.multiply_1\").output,model.get_layer(name=\"tf.math.multiply_2\").output, model.get_layer(name=\"tf.math.multiply_3\").output])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-07T07:34:29.984346Z","iopub.status.busy":"2024-03-07T07:34:29.983603Z","iopub.status.idle":"2024-03-07T07:34:37.275138Z","shell.execute_reply":"2024-03-07T07:34:37.273757Z","shell.execute_reply.started":"2024-03-07T07:34:29.984309Z"},"trusted":true},"outputs":[],"source":["for z in range(0,15):\n","    # Load one image and corresponding mask from the test dataset\n","    test_image = X_test[z]  # Replace X_test with your actual test dataset\n","    test_mask = y_test[z]  # Replace y_test with your actual test masks\n","\n","    # Reshape the image to match the input shape of the model\n","    test_image = np.reshape(test_image, (1,) + test_image.shape)\n","\n","    # Predict the segmentation mask for the test image\n","    predicted_mask = model.predict(test_image)[0]\n","    feature_map1, feature_map2, feature_map3, feature_map4 = modeller.predict(test_image)\n","    feature_map1 = tf.math.reduce_mean(feature_map1, axis=-1, keepdims=True)\n","    feature_map2 = tf.math.reduce_mean(feature_map2, axis=-1, keepdims=True)\n","    feature_map3 = tf.math.reduce_mean(feature_map3, axis=-1, keepdims=True)\n","    feature_map4 = tf.math.reduce_mean(feature_map4, axis=-1, keepdims=True)\n","\n","    # Convert the predicted mask values to binary (0 or 1)\n","    predicted_mask_binary = np.where(predicted_mask > 0.5, 1, 0) * 255\n","\n","    # Create subplots\n","    fig, axes = plt.subplots(1, len(feature_map1) + 6, figsize=(20, 4))\n","\n","    # Plot the test image\n","    axes[0].imshow(test_image[0], cmap='gray')\n","    axes[0].set_title('Test Image')\n","    axes[0].axis('off')\n","\n","    # Plot the ground truth mask\n","    axes[1].imshow(test_mask, cmap='gray')\n","    axes[1].set_title('Ground Truth Mask')\n","    axes[1].axis('off')\n","\n","    # Plot the predicted mask\n","    axes[2].imshow(predicted_mask_binary, cmap='gray')\n","    axes[2].set_title('Predicted Mask')\n","    axes[2].axis('off')\n","\n","    # Plot the feature maps (Set 1)\n","    for i, fmap in enumerate(feature_map1):\n","        axes[i + 3].imshow(fmap[:, :, 0], cmap='jet')\n","        axes[i + 3].set_title(f'Decoder 1')\n","        axes[i + 3].axis('off')\n","\n","    # Plot the feature maps (Set 2)\n","    for i, fmap in enumerate(feature_map2):\n","        axes[i + 3 + len(feature_map1)].imshow(fmap[:, :, 0], cmap='jet')\n","        axes[i + 3 + len(feature_map1)].set_title(f'Decoder 2')\n","        axes[i + 3 + len(feature_map1)].axis('off')\n","        \n","    # Plot the feature maps (Set 3)\n","    for i, fmap in enumerate(feature_map3):\n","        axes[i + 3 + len(feature_map1) + len(feature_map2)].imshow(fmap[:, :, 0], cmap='jet')\n","        axes[i + 3 + len(feature_map1) + len(feature_map2)].set_title(f'Decoder 3')\n","        axes[i + 3 + len(feature_map1) + len(feature_map2)].axis('off')\n","\n","    # Plot the feature maps (Set 4)\n","    for i, fmap in enumerate(feature_map4):\n","        axes[i + 3 + len(feature_map1) + len(feature_map2) + len(feature_map3)].imshow(fmap[:, :, 0], cmap='jet')\n","        axes[i + 3 + len(feature_map1) + len(feature_map2) + len(feature_map3)].set_title(f'Decoder 4')\n","        axes[i + 3 + len(feature_map1) + len(feature_map2) + len(feature_map3)].axis('off')\n","\n","    plt.tight_layout()\n","    plt.show()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":611834,"sourceId":1094872,"sourceType":"datasetVersion"},{"datasetId":786787,"sourceId":1351797,"sourceType":"datasetVersion"},{"datasetId":1209633,"sourceId":2021025,"sourceType":"datasetVersion"},{"datasetId":3597170,"sourceId":7730303,"sourceType":"datasetVersion"}],"dockerImageVersionId":30458,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":5}
